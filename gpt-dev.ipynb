{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GPT: from scratch, in code, spelled out.\n",
    "\n",
    "A development notebook for building a nanoGPT, following along with Andrej Karpathy's \"Let's build GPT: from scratch, in code, spelled out.\" tutorial on YouTube.\n",
    "\n",
    "Github: https://github.com/jt-thorpe/Lets-Build-GPT.git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-11 10:39:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  2.44MB/s    in 0.4s    \n",
      "\n",
      "2024-02-11 10:39:59 (2.44 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the tiny Shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "We need to process the dataset and get something workable:\n",
    "- so things such as getting the unique characters from the dataset; the ***vocabulary***\n",
    "- useful info; such as size of vocabulary\n",
    "- tokenise the vocab; there are many ways to do this\n",
    "  - character level (which we are doing)\n",
    "  - word level\n",
    "  - subword level; tikToken from OpenAI or SentencePiece from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  65\n",
      "Vocabulary: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Gather the vocabulary; that is, the unique characters in the text\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size: \", vocab_size)\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:  [21, 58, 5, 57, 1, 44, 56, 43, 43, 1, 56, 43, 39, 50, 1, 43, 57, 58, 39, 58, 43, 8, 8, 8]\n",
      "Decoded text:  It's free real estate...\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from character to numerical representation\n",
    "char_to_index = {char: index for index, char in enumerate(vocab)}\n",
    "index_to_char = {index: char for index, char in enumerate(vocab)}\n",
    "\n",
    "encode = lambda text: [char_to_index[char] for char in text]  # encoder: takes a string, returns a list of integers\n",
    "decode = lambda enc_text: ''.join([index_to_char[index] for index in enc_text])  # decoder: takes a list of integers, returns a string\n",
    "\n",
    "print(\"Encoded text: \", encode(\"It's free real estate...\"))\n",
    "print(\"Decoded text: \", decode(encode(\"It's free real estate...\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Our selection to use a character to integer mapping here reduces the size of our vocabulary considerably, but increases the length of our output considerably, compared to say per-word or per-subword encodings, which would increase the size of our vocabulary but decrease the length of our encoding outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text, store it in a tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:250])  # The same 250 characters as before, but now encoded as integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and validation set\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "#TODO: Why did Andrej use 0.9 for the training set here, why so much, 90% seems quite high? or is it not? Find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model, we will not be feeding all our data to the model in one go, this is way to computationally inefficient. We want to feed in chunks, or blocks, at a time and train on that.\n",
    "\n",
    "In addition, we want our model to be \"*used to* training on as small a context size (block_size) as 1, so in the future, even it only sees one character it \"*knows*\" how to make predictions on such a small context size, and in between, all the way up to a context of block_size. As such, we need to define our block size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our data block_size\n",
    "block_size = 8\n",
    "\n",
    "train_data[:block_size + 1]  # +1 ensures we actually have 8 training examples for our block_size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When input is tensor([18]) then target is 47\n",
      " When input is tensor([18, 47]) then target is 56\n",
      " When input is tensor([18, 47, 56]) then target is 57\n",
      " When input is tensor([18, 47, 56, 57]) then target is 58\n",
      " When input is tensor([18, 47, 56, 57, 58]) then target is 1\n",
      " When input is tensor([18, 47, 56, 57, 58,  1]) then target is 15\n",
      " When input is tensor([18, 47, 56, 57, 58,  1, 15]) then target is 47\n",
      " When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target is 58\n"
     ]
    }
   ],
   "source": [
    "# An example to show why +1 is necessary\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\" When input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, when we pass a block to the model, it is not just learning what the predicted next token is when it sees that block, it learns **all** of the possible combinations (in the order of the block still), for that block.\n",
    "\n",
    "So as demonstrated above, if we do not have +1 then we would only have 7 training examples for the model, for our block_size. This would mean we are missing the chance to learn the next predicted token for when there are 8 characters, hence we need an additional character (token).\n",
    "\n",
    "At each index, the i+1'th index is the target token the model will calculate a probability of appearing for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_size\n",
    "\n",
    "So we also need to define a batch_size, as when we are training our models on a GPU (which are designed for parallel processing), we want to be training lots of batches of chunks, all in parallel to each other as this is most efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "-----\n",
      "\n",
      "When input is tensor([24]) then target is 43\n",
      "When input is tensor([24, 43]) then target is 58\n",
      "When input is tensor([24, 43, 58]) then target is 5\n",
      "When input is tensor([24, 43, 58,  5]) then target is 57\n",
      "When input is tensor([24, 43, 58,  5, 57]) then target is 1\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]) then target is 46\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]) then target is 43\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) then target is 39\n",
      "When input is tensor([44]) then target is 53\n",
      "When input is tensor([44, 53]) then target is 56\n",
      "When input is tensor([44, 53, 56]) then target is 1\n",
      "When input is tensor([44, 53, 56,  1]) then target is 58\n",
      "When input is tensor([44, 53, 56,  1, 58]) then target is 46\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]) then target is 39\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]) then target is 58\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) then target is 1\n",
      "When input is tensor([52]) then target is 58\n",
      "When input is tensor([52, 58]) then target is 1\n",
      "When input is tensor([52, 58,  1]) then target is 58\n",
      "When input is tensor([52, 58,  1, 58]) then target is 46\n",
      "When input is tensor([52, 58,  1, 58, 46]) then target is 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]) then target is 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]) then target is 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) then target is 46\n",
      "When input is tensor([25]) then target is 17\n",
      "When input is tensor([25, 17]) then target is 27\n",
      "When input is tensor([25, 17, 27]) then target is 10\n",
      "When input is tensor([25, 17, 27, 10]) then target is 0\n",
      "When input is tensor([25, 17, 27, 10,  0]) then target is 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]) then target is 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]) then target is 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) then target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Set seed for reproducibility\n",
    "\n",
    "batch_size = 4  # How many independent sequences to process in parallel\n",
    "block_size = 8  # The length of each sequence (i.e. the context size for prediction)\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # Starting index for each sequence\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # The input sequences\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # The target sequences\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"\\nTargets: \")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x = torch.stack([data[i:i+block_size] for i in ix])  # The input sequences`\n",
    "\n",
    "`y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # The target sequences`\n",
    "\n",
    "By using the torch.stack here we are taking each 1d-tensor (e.g. [24, 43, 58,  5, 57,  1, 46, 43] etc...) and stack them as rows into one 4x8-d tensor, as shown in the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Set seed for reproducibility\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # A\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward phase of the network.\"\"\"\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch = 4, Time = 8, Channels = vocab_size = 65)-d tensor\n",
    "\n",
    "        # This is to make our tensor conform to how cross_entropy expects the input\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # Stretch thr 1st and 2nd dimension into one, with C now being the 2nd dimension\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate a sequence of new tokens.\n",
    "        \n",
    "        Works on the level of batches.\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities; a function to puhs values to 0-1 and sum to 1\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the output above is the random gibberish we get from our model while it is not trained. Lets now train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimiser will adjust the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimiser\n",
    "optim = torch.optim.Adam(m.parameters(), lr=1e-3)  # Popular rates are: 3e-4 (on smaller networks, higher learning rates are more feasible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9999, loss: 2.3796486854553223\n"
     ]
    }
   ],
   "source": [
    "# A standard training loop\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(f\"Step {steps}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Remember, each time we run the model the gradients adjust (i.e. the parameters from the previous run are still here) each time we run it, improving our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llo br. ave aviasurf my, mayo t ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;\n",
      "\n",
      "Whe, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty dedo bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "By bre ndy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n sar; my w, fredeeyong\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, pannd e Yolde Th li\n"
     ]
    }
   ],
   "source": [
    "# Let's see what our model has learned\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can See, it is still gibberish, but this gibberish is starting to take form! (So fucking cool...)\n",
    "\n",
    "### So why is this model still producing nothing useful?\n",
    "\n",
    "Well, in this iteration the tokens that we are learning are not \"talking\" to each other, they aren't impacting each other. We're just looking at the alst character for each token when we're making a prediciton.\n",
    "\n",
    "We need to have these tokens look at each other within their context, to make better predictions. This leads us onto building a **transformer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Transformer\n",
    "\n",
    "### A mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,2 # Batch, Time, Channels - The batches, the time steps and the channels (i.e. some information at each point in the sequence)\n",
    "x = torch.randn(B, T, C) # Random input\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 8 token in a batch, and currently they do not talk to each other. We want to \"couple\" them.\n",
    "\n",
    "In partiuclar, in a very specific way. For example, the token in the 5th position should not couple with tokens in the 6,7,8th position. It should **not** couple with future tokens- only those in the past- so the 1,2,3,4th tokens.\n",
    "\n",
    "We cannot get any new information from the future, as we are about to try and *predict* the future. I.e. we're trying to predict the 6th token, using the info of the previous tokens.\n",
    "\n",
    "So how does a token communicate with the previous tokens. The simplest way would be to average the preceeding elements.\n",
    "\n",
    "So if I am the 5th token, I want to take the channels that are information at my step, but also the channels for the 4,3,2,1st steps also. I average these up and get a feature vector that summarises me (the 5th token) in the context of my history.\n",
    "\n",
    "This a ***very weak form of interaction***- this interaction is very lossy- we lose a ton of information about the spacial arrangements of those tokens, but this is okay for now.\n",
    "\n",
    "So what do we want to do for now:\n",
    "- for every single batch element independently (every t'th token)\n",
    "- calculate the average of all the vectors of the previous steps plus this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want `x[b,t] = mean_{i<=t} x[b,i]`\n",
    "xbow = torch.zeros((B, T, C))  # bow = bag of words; a common phrase when averaging over words\n",
    "\n",
    "# Loop not efficient, but good for understanding\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # Average over the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So 1st row of xbow is an average of the 1st row of x\n",
    "- the 2nd row of xbow is an average of the 1st and 2nd row of x\n",
    "- 3rd row of xbow is an average of the 1st, 2nd and 3rd row of x\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here comes the trick!\n",
    "\n",
    "For efficiency, we dont need to loop, we can do this with matrix-multi very efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Another toy example\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with some matrix magic, we can take a lower-triangle matrix of 1s and 0s (i.e. 0s in all i,j index above mid diag)\n",
    "\n",
    "So when we now do he dot product, we're basicially summing the rows as described above in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Another toy example\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We now have efficient summing of rows without any looping, using matrix magic. Bon ap. With a small adjustment, we can then make this into the averages we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Another toy example\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)  # Normalise the rows\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that c is now rows of averages of each row plus its previous.\n",
    "\n",
    "### Vectoring the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4,8,2 # Batch, Time, Channels - The batches, the time steps and the channels (i.e. some information at each point in the sequence)\n",
    "x = torch.randn(B, T, C) # Random input\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Version 1\"\"\"\n",
    "# We want `x[b,t] = mean_{i<=t} x[b,i]`\n",
    "xbow = torch.zeros((B, T, C))  # bow = bag of words; a common phrase when averaging over words\n",
    "\n",
    "# Loop not efficient, but good for understanding\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # Average over the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Version 2\"\"\"\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)  # Normalise the rows\n",
    "xbow2 = wei @ x  # (B,T,T) @ (B,T,C) ----> (B,T,C) (PyTorch will broadcast (add) the first dimension)\n",
    "\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Version 3\"\"\"\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # Set the upper triangle to -inf : \"The future cannot communicate with the past\"\n",
    "wei = F.softmax(wei, dim=1)  # Softmax over the time dimension\n",
    "xbow3 = wei @ x\n",
    "\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]),\n",
       " tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]),\n",
       " tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0], xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why \"Version 3\" over the others?\n",
    "\n",
    "- Well, we first start out with the weight matrix, wei, as all 0s;\n",
    "- we then fill the weight matrix with -inf for all **future** tokens- we dont want to consider any values from the future\n",
    "- the weights in the wei matrix can be thought of each tokens affinity for another token- they are data dependent- some tokens will become interested in other tokens, and to varying amounts\n",
    "- so when we normalise (softmax) and then sum, we're agregating their values absed on how interesting they find each other\n",
    "- this is **ATTENTION**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
